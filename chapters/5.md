> [!WARNING]
> This page is a quick summary of the slides.
> - Make sure you study from the [Book material](/resources/book) first.
> - Ensure that you also practice Book Activities.

# Chapter 5:<br>Information Theory and Lossless Compression

::: details Resources
This Chapter covers the following:
- [Book Material](https://1drv.ms/b/s!AqxAKvx2i8TygR4-ZMB1OnA8CxuH?e=CmbP8O)
- [Session 6 Slides](https://1drv.ms/b/s!AqxAKvx2i8TygQZM-BKhuJfHB7Rs?e=JupOsv)
- [Session 7 Slides](https://1drv.ms/b/s!AqxAKvx2i8TygQdkq4DYrb-GRePV?e=fGPteK), starts [@Huffman Coding](#huffman-coding)
- [Video Lecture Pt.1 (YT)](https://www.youtube.com/watch?v=SAscMEbJFco)
- [Video Lecture Pt.2 (YT)](https://www.youtube.com/watch?v=2K6qX2SCntQ)
:::

> [!NOTE] Summary
> Summary to be filled

## Introduction

- Examines the **source of bits** transmitted through a channel.
- Covers **source coding**, which includes:
  - **Digitizing** the source signal.
  - **Compressing** the digital signal.

## Compression

### Why Compress?
- **Large storage requirements** for uncompressed media:
  - 1 min of **CD-quality** music ≈ **10 MB**.
  - 1 min of **HD video** (1280x720) ≈ **4 GB**.
- **Compression reduces size** while retaining key information.

### Types of Compression
- **Lossless compression**: Allows **exact reconstruction** of data.
- **Lossy compression**: **Irreversible loss** of some information but more effective reduction.

### Common Compression Formats
- **Lossless**: ZIP, PNG, GIF, TIFF.
- **Lossy**: MP3, JPEG, MPEG, AVC, AAC.

## Lossless Compression

### Key Techniques
- **Run-Length Encoding (RLE)**: Stores repeated data efficiently.
- **Lempel–Ziv–Welch (LZW)**: Used in GIF and ZIP.
- **Huffman Coding**: Reduces average bit length by assigning shorter codes to frequent symbols.

### Efficiency Considerations
- Best for **text and structured data**.
- Compression ratio **depends on redundancy**.
- Can sometimes **increase file size** if data is already efficient.

## Lossy Compression

### Key Features
- Used for **audio, images, video**.
- Often **improves quality** by **removing noise**.
- Achieves higher **bit-rate reductions** compared to lossless techniques.

### Examples
- **MP3, AAC**: Audio compression.
- **JPEG, JPEG2000**: Image compression.
- **MPEG, AVC**: Video compression.

### Hybrid Compression
- Some formats combine **lossy + lossless** techniques.
- **JPEG** uses **Huffman coding** and **RLE** post-lossy encoding.

## Information Theory

- Studies **quantification & communication of information**.
- Originally developed by **Shannon**.
- Defines **source alphabet** (set of possible symbols).

### Examples of Source Alphabets
- **Text data**: Alphabet + punctuation.
- **Sports results**: Win, lose, draw.

## Fixed-Length Codes

### Definition
- Each symbol has a **fixed** number of bits.

### Example
- **27 symbols** (A-Z + space) require:
  $$ \log_2(27) \approx 4.75 \text{ bits per symbol} $$
- **37 symbols** (A-Z, space, 0-9) require:
  $$ \log_2(37) \approx 5.2 \text{ bits per symbol} $$

## Variable-Length Codes

### Definition
- Assigns **shorter codes** to frequent symbols.
- **Reduces average bit length**.

### Unique Decodability
- A valid code must be **uniquely decodable**.
- **Instantaneously decodable**: Symbols can be decoded as they arrive.

### Example of Efficient Coding
- If **draws occur more frequently** in sports results:
  - Assign **shorter code** to "draw".
  - Reduces **average bits per symbol**.

## Code Trees

- Used to **construct variable-length codes**.
- Ensures **no code word is a prefix** of another.
- Decoding follows **branches of the tree**.

## Source Symbol Probabilities

- **Optimal coding** depends on symbol probabilities.
- **Average code length**:
  $$ \bar{l} = \sum_{i=1}^{n} l_i p_i $$
  where \( l_i \) is symbol length and \( p_i \) is probability.

### Example
- If **visibility probabilities** are:
  - Good: **0.5**
  - Moderate: **0.3**
  - Poor: **0.1**
  - Very Poor: **0.1**
- Assign **shorter codes** to **more frequent symbols**.

## Source Entropy and Information

### Shannon Entropy
- **Minimum possible bits per symbol**:
  $$ H = -\sum_{i=1}^{n} p_i \log_2(p_i) $$

### Code Efficiency
- Efficiency of a code:
  $$ E = \frac{H}{\bar{l}} $$
- The **closer \( E \) is to 1**, the **more efficient** the code.

## Huffman Coding

> [!NOTE]
> This is the start of [Session 7 Slides](https://1drv.ms/b/s!AqxAKvx2i8TygQdkq4DYrb-GRePV?e=fGPteK)

### Constructing a Huffman Code

- **Huffman coding** assigns **shorter codes** to more **frequent symbols**.
- The process involves:
  1. **Ordering symbols** by probability.
  2. **Combining lowest-probability symbols** iteratively.
  3. **Building a binary tree** with 0/1 assignments.
  4. **Generating codewords** from the tree.

### Efficiency Calculation
- **Average Code Length**:
  $$ \bar{l} = \sum_{i=1}^{n} l_i p_i $$
- **Entropy Calculation**:
  $$ H = -\sum_{i=1}^{n} p_i \log_2 p_i $$
- **Efficiency**:
  $$ E = \frac{H}{\bar{l}} $$

### Compressing Files with Huffman Codes
- Uses symbol **frequencies** to assign **optimized codes**.
- **Reduces storage size** by encoding **more common characters** efficiently.
- Requires storing a **Huffman tree** for decompression.
- **Overhead**: Small files may not benefit due to **table storage requirements**.

### Limitations of Huffman Coding
- Assumes **symbols are random**.
- Does not exploit **long runs of identical symbols**.
- Requires **pre-analysis** of data.
- Not always the best choice for **real-time compression**.

## Run-Length and Lempel–Ziv–Welch (LZW) Coding

### Run-Length Encoding (RLE)
- Replaces **repeated symbols** with a **count and symbol**.
- Example:
  - **Uncompressed**: `CCCCDDDDDDDAEEEEEEAAAD`
  - **RLE Compressed**: `C4D6A1E7A3D1`
- **Advantages**:
  - Works **on-the-fly**, minimal processing delay.
  - **Simple implementation**.
- **Disadvantages**:
  - Ineffective for **non-repetitive data**.
  - Works best for **images and simple text patterns**.

### Lempel–Ziv–Welch (LZW) Coding
- Detects **repeating patterns** and assigns them **shorter codes**.
- Creates a **coding table (dictionary)** dynamically.
- Commonly used in **ZIP, GIF, and TIFF formats**.

### The LZW Coding Table
- Uses **12-bit codes** (4096 entries).
- **First 256 entries** represent **single-byte symbols**.
- New entries **store repeated patterns**.

### LZW Coding Principles
- **Encoding Process**:
  1. Read input **byte-by-byte**.
  2. Find the **longest match** in the dictionary.
  3. **Output code** for that match.
  4. Add **new sequence** to the dictionary.
- **Decoding Process**:
  - Rebuilds **dictionary** dynamically while decoding.
  - Avoids storing the dictionary explicitly.
  - Handles **edge cases** where a symbol appears for the first time.

### LZW Example
- **Uncompressed**: `BCBCBBBACBCBCB`
- **Compressed**: `66 67 256 66 259 65 257 262 66`
- **Table is built dynamically**, allowing for compact encoding.


## Video Lecture

<iframe width="720" height="480" src="https://www.youtube.com/embed/SAscMEbJFco?si=rPcqQcIBbgZMy8SM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<br>
<iframe width="720" height="480" src="https://www.youtube.com/embed/2K6qX2SCntQ?si=2aMfwb_u_SQmIzv4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Metawally

### Session 6

<iframe src="https://drive.google.com/file/d/1jTk4kk9W6JZJ8mtg2jmTBu-DXXL2FTf5/preview" width="720" height="480" allow="autoplay" allowfullscreen></iframe>

### Session 7

<iframe src="https://drive.google.com/file/d/1dfX9Jr0yrcvUqNbh7egSXIl7hwGgLqhB/preview" width="720" height="480" allow="autoplay" allowfullscreen></iframe>

### Activities

<iframe src="https://drive.google.com/file/d/14uIPQMvchIkRUahF8p_B3aEDOxjM4ls_/preview" width="720" height="480" allow="autoplay" allowfullscreen></iframe>

